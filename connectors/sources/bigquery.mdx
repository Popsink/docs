---
title: 'BigQuery Source'
description: 'Stream data from Google BigQuery tables with change history'
icon: 'cloud'
---

The BigQuery Source integration in Popsink allows you to seamlessly ingest data from a BigQuery database into Popsink pipelines. With this connector, you can extract structured data for processing, transformation, and further downstream operations.

## Prerequisites

<Warning>
**Change History Required:** The source table must be configured with change history enabled:
```sql
ALTER TABLE `project.dataset.table`
SET OPTIONS (
  enable_change_history = TRUE
);
```
</Warning>

## Permissions

The service account used for the connector requires the following BigQuery permissions:

<ParamField path="bigquery.tables.getData" type="permission" required>
  Read table data and access change history
</ParamField>

### Setting up permissions

```sql
-- Grant the BigQuery Data Viewer role
GRANT `roles/bigquery.dataViewer` 
ON TABLE `project.dataset.table` 
TO "serviceAccount:your-service-account@project.iam.gserviceaccount.com";

-- Or grant specific permissions
GRANT `bigquery.tables.getData`
ON TABLE `project.dataset.table`
TO "serviceAccount:your-service-account@project.iam.gserviceaccount.com";
```

## Key Features

<CardGroup cols={2}>
  <Card title="Change History Tracking" icon="clock-rotate-left">
    Leverages BigQuery's native change history feature to capture incremental updates to tables.
  </Card>
  
  <Card title="High Performance" icon="gauge-high">
    Optimized for BigQuery's architecture, efficiently reading large datasets with minimal overhead.
  </Card>
  
  <Card title="Flexible Querying" icon="magnifying-glass">
    Support for custom SQL queries to extract exactly the data you need.
  </Card>
  
  <Card title="Partitioned Tables" icon="layer-group">
    Efficiently handles partitioned tables for optimized data extraction.
  </Card>
</CardGroup>

## Configuration

### Connection Settings

<ParamField path="Project ID" type="string" required>
  Your Google Cloud project ID
</ParamField>

<ParamField path="Dataset" type="string" required>
  The BigQuery dataset containing your tables
</ParamField>

<ParamField path="Table" type="string" required>
  The table name to extract data from
</ParamField>

<ParamField path="Service Account JSON" type="string" required>
  The JSON key file for your service account with appropriate permissions
</ParamField>

### Advanced Settings

<ParamField path="Query" type="string">
  Optional custom SQL query to filter or transform data during extraction
</ParamField>

<ParamField path="Change History Interval" type="duration" default="5m">
  How frequently to check for changes (e.g., `5m`, `1h`)
</ParamField>

<ParamField path="Batch Size" type="number" default="1000">
  Number of records to fetch per request
</ParamField>

## Setup Guide

<Steps>
  <Step title="Create a Service Account">
    In Google Cloud Console, create a new service account with BigQuery permissions:
    
    1. Navigate to **IAM & Admin > Service Accounts**
    2. Click **Create Service Account**
    3. Grant the **BigQuery Data Viewer** role
    4. Generate and download the JSON key
  </Step>
  
  <Step title="Enable Change History">
    Modify your BigQuery table to enable change history:
    ```sql
    ALTER TABLE `your-project.your-dataset.your-table`
    SET OPTIONS (
      enable_change_history = TRUE
    );
    ```
    
    <Note>
    Change history must be enabled **before** the connector can track changes. Existing changes before enabling this feature won't be captured.
    </Note>
  </Step>
  
  <Step title="Configure the Connector">
    In Popsink, add a new BigQuery source connector with:
    - Your project ID
    - Dataset and table names
    - Service account JSON key
  </Step>
  
  <Step title="Test and Deploy">
    Test the connection and start streaming data from BigQuery to your target destination.
  </Step>
</Steps>

## Best Practices

<Tip>
**Partition Your Tables:** Use partitioned tables in BigQuery to improve query performance and reduce costs.
</Tip>

<Tip>
**Monitor Quotas:** Be aware of BigQuery API quotas and rate limits. Adjust your sync frequency accordingly.
</Tip>

<Tip>
**Use Table Snapshots:** For large historical loads, consider using BigQuery table snapshots for consistent point-in-time data.
</Tip>

<Tip>
**Cost Optimization:** Use column selection and filters to minimize the amount of data scanned and transferred.
</Tip>

## Limitations

<Warning>
**Change History Limitations:**
- Change history data is retained for 7 days by default
- Not all table types support change history (e.g., external tables)
- Schema changes may require reinitialization
</Warning>

## Troubleshooting

<AccordionGroup>
  <Accordion title="Change history not enabled error" icon="circle-exclamation">
    Ensure you've run the `ALTER TABLE` command to enable change history. Verify with:
    ```sql
    SELECT * FROM `your-project.your-dataset.INFORMATION_SCHEMA.TABLE_OPTIONS`
    WHERE table_name = 'your-table' AND option_name = 'enable_change_history';
    ```
  </Accordion>
  
  <Accordion title="Permission denied errors" icon="lock">
    Verify that your service account has the `bigquery.tables.getData` permission on the specific table, not just the dataset.
  </Accordion>
  
  <Accordion title="High costs" icon="dollar-sign">
    Monitor your BigQuery costs:
    - Reduce sync frequency
    - Use column selection to limit scanned data
    - Implement incremental extraction
    - Consider using materialized views
  </Accordion>
</AccordionGroup>

## Related Resources

<CardGroup cols={2}>
  <Card title="BigQuery Target" icon="upload" href="/connectors/targets/bigquery">
    Stream data back to BigQuery
  </Card>
  
  <Card title="Snowflake Target" icon="snowflake" href="/connectors/targets/snowflake">
    Load BigQuery data into Snowflake
  </Card>
</CardGroup>
